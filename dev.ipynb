{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9dea42f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e39cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.core.training.data import CustomDataset\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0daddfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"app/core/datasets/Persian OCR Dataset - kaggle farboodi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2daeddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CustomDataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe5ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شانلیاورفه باشه از\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAtCAIAAADOcHVEAAACqklEQVR4Ae1Y2W4DQQhrqv7/L6dESHTKNRfsZhTytMNgY+wkrfJ4Pp9f9TrHge9zpJbSlwMV2GHvgwqsAjvMgcPk1iesAjvMgcPk1iesAjvMgcPk1iesAlty4PF4LOHeCHTNCkOfsGukxHofqDmKKoSnH1jImNgwzmXbN7MfWOCvw/tyx6MKlD0+dL+za1E/sH0Rn8kgrQ95D10aWIjig+JfyAwskqh25Z/2AM/YHe4sExHOz7bIW0QOsiqzOzKLLFoeWDdhi8ivz6r32cZvwYW7Ro+LxM5BncpX4iBSFTT4NlGxy0UYyubCcWeLZSVdINPZ7ZcNSmCyabCyr2Zw0FTbrKrZ/ikx+81hgWXsOcJp9Vh13zJErWGBWQJHKr4kdqsEJmcwjDwuQCTJbAWGWnO734cW0NIw22/xTNXVoUpgQKq2WsOmmi2S2aGSh8mAzOglm51xgGK3WGEkbBy7zTvqf5xBjarS0iHVA1wWW7jkp368wqNsa0kIQkXqpyuqUI/zQCjoaYFUhyI9Iw+2OcWWByDUKamkMIZ9wWVJwqYqJMhHybkqULZZtCocmscZoJlIGIrq1nSnvkzFgDgiPjC5Nm5L4+FIz2xP5ovVxlB03IS3PHJ0S063VJQVYKMiMeMDoagHK9QPR3rmWOuC9X3U0fHrdh+Ufzra/G/XlyTA2fGd0wI3/v005ayRZNwttM6aztUtUuXQV2BM5Tt8SaKkQCXOjs6V9AuaURU9yJ7Uyt8ft7sUqOsliXFo4QqUdN8i2Iaau83qapvFv8A2iT4ETpHTw8WLV2AXG747TvkvcZey8JkOVGCZ7iZwV2AJpmZSVmCZ7iZwV2AJpmZSVmCZ7iZwV2AJpmZSVmCZ7iZwV2AJpmZSVmCZ7iZwV2AJpmZSVmCZ7iZw/wIFz+qjzGlGbAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=144x45>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 7891\n",
    "print(a[i][1])\n",
    "a[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e4afaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAAAtCAIAAACF/fGAAAAE9klEQVR4Ae1a23YcMQhrevr/v5ySOFUIYHwZ27vbUV7qASGBPHT3NH17f3//xR86QAd2OvB7Jzm56QAd+HCAa8b3gA5sd4Brtt1iCtABrhnfATqw3QGu2XaLKUAHuGZ8B+jAdgf+bFegwKwDb29vKF37exdhXkuIPnkIHeCnWWjL44P7dqzMpvkfP+3/3gE/zZ7uhvUC8DPn6a5nqiGu2ZRte4q4YHt8fTwrvzQ+/g5KB9yxZ7mJDX3w02yDqRcoV31LLEu7iu3CQCz9cIBr9lzvgazHwt1Yy5Y7pT+NBblwilz3JbKNL43Gu5cY6dWbvOi5lF9hmCifKBm9oysTjWrtwGdrNjfbAdMnjHhsVz1O6r/+e/ChCdOFhQ3lcsA5FAqDMoKeIsRMB0f7+Zzg+xeP07pLCld+RZGGtBfLHS/kE7SruhKeCfViS38huu0vKa+CKSyPIUmYQrl5sUIGYEIqZB9yMIPk/c91ODp19mnmOzADGIDJmkcDzh/D2uJXmErYDN48JoVhaq586KaHwGgSjaEcB2Dyg+D1D8DCDHIEH37ob6kf6Yeq1YpRAq5lPc/YmiXUkCxX5ZVMBHgTTyQ8shmByvWumloLAaO3COlSiEc5wAEd7Dkbx4QHPz3lBzDSj1dB0PTvkYigBBF9yLMamZx/rJkwXiT115xoX9RKmE3q2br6tDl4RUzbE4/LLW1aVwBe9/qMnrPfkGbbhuqKlqEKH3+sWYjQwWb3pd3dTeuWes6HuxK5opj3lmCSVI0TV/Oh/e+nBu6PC5MBQ8jE/2l+/Wmyo49e1zDU2iiwUt4kMZy1x+s8Y2umZ1jSkx8AERwg5CNINQ9DtSE4DDZ1c4DnlAiC+ZvkmWt4EErJJ/3X5uizYSspFNaYTZV57KyCiinveUxqk1TI7PE+gsIkBYw+fP/TWalsWpPAvHbC5sG6reSccIZVXihh8GDDmdQCKSRN2BIhKCaHplBSW1KYpZ8KJU1ycOqSEtSRkKcGAyeqciqPR2F+yGl17cr/BQLVZt85oMaDuB6geUZVLio8OQA8uWJOkteWbKdQD5VgCpvpChImrjmBQTCkMlk8dh6EU3rwWp3lIQxsyXQozDE1KsTBkx9WrlmuhCwGa/baBIDz+qG/q+taYDg2YE2oFkeH/jBR4kl0ZDmhJu88N3toAnKhgTUbfRHDzkZJ8u4nss/QFUyY6J8l4gAMDG/TWFTDDJEYztHH7zWTbkS4aOvOSgS8OoUgDgaMuBx0KifRVUvOWtoQ6tTCroTWs2ktacMDTG83fyx2aZf6DTRI46TOan4DW/j4423Q8qFG0pOp1cgkpVUEhip9Lhgf0bW1cyKdpEK2/gYMc8iGScMsg00PawbqwhDTBIj5gglry73k2fDuAjrdB2oS1dJWDanZekgKplRpvI9AsXZIpJNUyDaqrvkNoR7KpJ7kcXTYfW2HNiYGaryH5Vk9hXZAnwvGR3RteA7WLMTlwUQ4SeWc17OJdJK6rkuGRzmQX2ue3drzmjXb2iLJ6cCrOzDzv0BefWb2TwcOO8A1O2w45e7oANfsjrfOmQ87wDU7bDjl7ugA1+yOt86ZDzvANTtsOOXu6ADX7I63zpkPO8A1O2w45e7oANfsjrfOmQ87wDU7bDjl7ugA1+yOt86ZDzvANTtsOOXu6ADX7I63zpkPO8A1O2w45e7owF/eP/Apc6gGkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=290x45>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(os.path.join(a.images_path, a.images[1])).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f445b",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0e3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTModel,\n",
    "    BertLMHeadModel,\n",
    "    AutoTokenizer,\n",
    "    TrOCRProcessor,\n",
    "    ViTImageProcessor,\n",
    "    EncoderDecoderModel,\n",
    "    BertConfig\n",
    ")\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485ebf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at app/core/models/trocr-base-printed and are newly initialized: ['embeddings.cls_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at app/core/models/bert-fa-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"app/core/models/trocr-base-printed\",\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"app/core/models/bert-fa-base-uncased\"),\n",
    "    use_fast=True,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"app/core/models/bert-fa-base-uncased\")\n",
    "encoder = ViTModel.from_pretrained(\"app/core/models/trocr-base-printed\")\n",
    "decoder = BertLMHeadModel.from_pretrained(\n",
    "    \"app/core/models/bert-fa-base-uncased\",\n",
    "    is_decoder=True,\n",
    "    add_cross_attention = True,\n",
    ")\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder).to(\"cuda\")\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.generation_config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.bos_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8e0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 384, 384\n",
    "dummy_image = Image.fromarray(np.uint8(np.random.rand(height, width, 3) * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd987ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 384, 384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values = processor(images=[dummy_image, dummy_image], return_tensors=\"pt\", device=\"cuda\").pixel_values\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5e6eb734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'و و و و و و و و و و و و و و و و و و و و'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(pixel_values.to(\"cuda\"))\n",
    "processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"احمد محسن\", \"سلام علیکم و رحمه الله\"]\n",
    "labels = processor.tokenizer(text, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b7abfccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] احمد محسن [SEP] [PAD] [PAD] [PAD]',\n",
       " '[CLS] سلام علیکم و رحمه الله [SEP]']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(labels, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7b3fb8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "45ecd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(pixel_values=pixel_values.to(\"cuda\"), labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "50dbeac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3094, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "766e64af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379,\n",
       "         1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379],\n",
       "        [   2, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379,\n",
       "         1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "77ac77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'و و و و و و و و و و و و و و و و و و و و'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30ea0213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='app/core/models/trocr-base-printed', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
